# Bible Quote Accuracy Analysis

This test suite evaluates AI models' ability to accurately quote Bible verses across multiple languages and translations.

## Overview

The experiment tests:
- **100 Bible verses** ranging from famous (John 3:16) to very obscure passages
- **30 Bible versions** across different languages (English, Greek, Hebrew, Latin, Spanish, Chinese, Arabic, etc.)
- **12 AI models** from major providers (Google, OpenAI, Anthropic, Meta, Qwen, DeepSeek, Grok)

## Quick Start

### Prerequisites

1. Python 3.8 or higher
2. requesty.ai API key (set as `REQUESTY_API_KEY` environment variable)

### Installation

```bash
cd tests
pip install -r requirements.txt
```

### Setting Up API Key

```bash
export REQUESTY_API_KEY='your-api-key-here'
```

### Running the Full Pipeline

#### Option 1: Run Everything (Recommended for First Time)

```bash
# 1. Collect baseline data (if quote-bible skill is available)
python collect_baseline.py

# 2. Test all models (WARNING: This will make many API calls and incur costs!)
python test_models.py

# 3. Analyze results
python analyze_results.py

# 4. Generate report
python generate_report.py
```

#### Option 2: Quick Test (Recommended for Development)

Run a limited test first to verify everything works:

```bash
python test_models.py
# When prompted, choose 'y' for limited test
# This tests only 5 verses × 3 versions to verify setup
```

## Project Structure

```
tests/
├── consts/
│   ├── verses.py          # 100 test verses with difficulty ratings
│   └── languages.py       # 30 Bible versions and 12 AI models
├── collect_baseline.py    # Collect baseline quotes (placeholder)
├── test_models.py         # Test AI models via requesty.ai
├── analyze_results.py     # Analyze and compare results
├── generate_report.py     # Generate markdown report
├── database.py           # SQLite database management
├── requirements.txt      # Python dependencies
└── README.md            # This file
```

## Data Storage

All data is stored in `bible_quote_accuracy.db` (SQLite database) with the following tables:

- `verses` - Test verses with difficulty ratings
- `bible_versions` - Bible translations/versions
- `ai_models` - AI models being tested
- `baseline_quotes` - Reference quotes (if available)
- `model_quotes` - Quotes generated by AI models
- `analysis_results` - Comparison metrics and analysis

## Evaluation Metrics

Each quote is evaluated using:

1. **Exact Match**: Perfect match after text normalization
2. **Similarity Score**: 0.0 to 1.0 using SequenceMatcher
3. **Word Error Rate (WER)**: Ratio of word-level edits needed
4. **Character Error Rate (CER)**: Ratio of character-level edits needed
5. **Levenshtein Distance**: Number of single-character edits required

## Test Verses

Verses are categorized by familiarity/difficulty (1-10 scale):

- **Difficulty 1-2**: Famous verses (John 3:16, Genesis 1:1, etc.)
- **Difficulty 3-5**: Moderately known verses
- **Difficulty 6-7**: Less common verses
- **Difficulty 8-10**: Very obscure verses

See `consts/verses.py` for the complete list.

## Bible Versions

30 versions selected to represent:

- **Common English**: NIV, KJV, ESV, NLT, NASB, WEB, CSB
- **Less Common English**: NRSV, ASV, YLT
- **Source Languages**: Greek NT, Hebrew OT, Latin Vulgate
- **Gateway Languages**: Spanish, French, German, Chinese, Arabic, Swahili, etc.
- **Regional Languages**: Indonesian, Thai, Vietnamese, Korean, Amharic, etc.
- **Rare Languages**: Quechua, Navajo, Nepali

See `consts/languages.py` for the complete list.

## AI Models Tested

12 models across major providers:

### Google
- gemini-2.0-flash-exp (low tier)
- gemini-exp-1206 (high tier)

### OpenAI
- gpt-4o-mini (low tier)
- gpt-4o (high tier)

### Anthropic
- claude-3-5-haiku-20241022 (low tier)
- claude-3-5-sonnet-20241022 (high tier)

### Qwen
- qwen-2.5-7b-instruct (low tier)
- qwen-max (high tier)

### DeepSeek
- deepseek-chat (low tier)
- deepseek-reasoner (high tier)

### Meta
- llama-3.1-8b-instruct (low tier)
- llama-3.1-70b-instruct (high tier)

## Cost Estimation

**WARNING**: Running the full test suite will make many API calls.

Estimated API calls for full test:
- **Verses**: 100
- **Versions**: 30
- **Models**: 12
- **Total calls**: 100 × 30 × 12 = **36,000 API calls**

For development/testing, use the limited test option (5 verses × 3 versions × 12 models = 180 calls).

## Output

After running all scripts, you'll have:

1. **Database**: `bible_quote_accuracy.db` with all raw data
2. **Report**: `BIBLE_QUOTE_ACCURACY_REPORT.md` with comprehensive analysis

The report includes:
- Executive summary
- Model performance comparison
- Language coverage analysis
- Verse difficulty analysis
- Detailed examples (best and worst cases)
- Methodology
- Recommendations

## Customization

### Adding More Verses

Edit `consts/verses.py` and add tuples in the format:
```python
("Reference", "Category", difficulty_score)
```

### Adding More Bible Versions

Edit `consts/languages.py` and add tuples in the format:
```python
("CODE", "Name", "Language", "Script", "Family", rarity_score)
```

### Adding More Models

Edit `consts/languages.py` in the `MODELS_TO_TEST` list:
```python
("model-id", "provider", "Model Name", "tier")
```

## Troubleshooting

### API Key Issues

```bash
# Verify key is set
echo $REQUESTY_API_KEY

# If not set
export REQUESTY_API_KEY='your-key'
```

### Database Issues

```bash
# Remove database to start fresh
rm bible_quote_accuracy.db

# Re-run pipeline
python test_models.py
```

### requesty.ai API Configuration

The `test_models.py` script uses a generic REST API format. If the actual requesty.ai API differs:

1. Check the official requesty.ai documentation
2. Update the `RequestyAIClient` class in `test_models.py`
3. Adjust the `base_url`, `headers`, and `payload` format as needed

## Notes

### Baseline Collection

The `collect_baseline.py` script is a placeholder for collecting reference quotes. Options:

1. **quote-bible skill**: If available, integrate it
2. **Bible API**: Use a service like scripture.api.bible (requires API key)
3. **Manual**: Manually add baseline quotes to the database

Without baseline quotes, the analysis will still work but will compare models against each other rather than against authoritative sources.

### Rate Limiting

The test script includes a 0.1 second delay between API calls. Adjust in `test_models.py` if needed:

```python
time.sleep(0.1)  # Adjust this value
```

## Future Enhancements

Potential improvements:
- [ ] Integrate with Bible API for baseline quotes
- [ ] Add support for multi-verse passages
- [ ] Test paraphrase detection
- [ ] Add visualization (charts/graphs)
- [ ] Export results to CSV/JSON
- [ ] Add confidence intervals
- [ ] Test with different temperature settings
- [ ] Evaluate context understanding
- [ ] Test version-specific requests

## Contributing

To contribute:
1. Add new verses, versions, or models to the consts files
2. Run the test pipeline
3. Generate updated report
4. Submit results

## License

See project root LICENSE file.
